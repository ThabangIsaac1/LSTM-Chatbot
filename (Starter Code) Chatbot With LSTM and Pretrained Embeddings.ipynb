{"cells":[{"cell_type":"markdown","metadata":{"id":"pJAWnBFlkE2w"},"source":["# LSTM Bot\n","\n","## Project Overview\n","\n","In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n","\n","Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n","\n","\n","\n","---\n","\n","\n","\n","A sequence to sequence model (Seq2Seq) has two components:\n","- An Encoder consisting of an embedding layer and LSTM unit.\n","- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n","\n","The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n","\n","## Dependencies\n","\n","- Pytorch\n","- Numpy\n","- Pandas\n","- NLTK\n","- Gzip\n","- Gensim\n","\n","\n","Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n","\n","- https://pytorch.org/text/stable/datasets.html\n","\n","\n","\n"]},{"cell_type":"code","source":["!pip uninstall torchtext -y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zSHW1cMuYwgq","executionInfo":{"status":"ok","timestamp":1680219162328,"user_tz":-120,"elapsed":869,"user":{"displayName":"tech team","userId":"10297070148189427994"}},"outputId":"fb5d2789-5490-41a6-9b12-066e0b8c65cc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torchtext 0.14.1\n","Uninstalling torchtext-0.14.1:\n","  Successfully uninstalled torchtext-0.14.1\n"]}]},{"cell_type":"code","source":["!pip install torchtext==0.9.0 torch==1.8.0 torchvision==0.9.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HPn3wlqVO9-i","executionInfo":{"status":"ok","timestamp":1680219226686,"user_tz":-120,"elapsed":64363,"user":{"displayName":"tech team","userId":"10297070148189427994"}},"outputId":"a8ed884b-ef1d-49d6-d817-b531f8c7e30c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchtext==0.9.0\n","  Downloading torchtext-0.9.0-cp39-cp39-manylinux1_x86_64.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch==1.8.0\n","  Downloading torch-1.8.0-cp39-cp39-manylinux1_x86_64.whl (735.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.5/735.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.9.0\n","  Downloading torchvision-0.9.0-cp39-cp39-manylinux1_x86_64.whl (17.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtext==0.9.0) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchtext==0.9.0) (2.27.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchtext==0.9.0) (1.24.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.8.0) (4.5.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from torchvision==0.9.0) (8.4.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.9.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.9.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.9.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.9.0) (3.4)\n","Installing collected packages: torch, torchvision, torchtext\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.1+cu116\n","    Uninstalling torch-1.13.1+cu116:\n","      Successfully uninstalled torch-1.13.1+cu116\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.14.1+cu116\n","    Uninstalling torchvision-0.14.1+cu116:\n","      Successfully uninstalled torchvision-0.14.1+cu116\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.8.0 torchtext-0.9.0 torchvision-0.9.0\n"]}]},{"cell_type":"code","source":["import gensim\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import gzip\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from nltk.corpus import brown\n","from torchtext.legacy.data import Field, BucketIterator, Dataset\n","from torchtext.data.utils import get_tokenizer\n","from sklearn.model_selection import train_test_split\n","import json\n","import requests\n","from torchtext.legacy.data import Example\n","\n","nltk.download('brown')\n","nltk.download('punkt')\n","\n","# Load Brown embeddings\n","model = gensim.models.Word2Vec(brown.sents())\n","model.save('brown.embedding')\n","w2v = gensim.models.Word2Vec.load('brown.embedding')\n","\n","\n","def prepare_text(sentence):\n","    tokenizer = get_tokenizer('basic_english')\n","    tokens = tokenizer(sentence)\n","    return tokens\n","\n","def download_file(url, filename):\n","    response = requests.get(url)\n","    open(filename, 'wb').write(response.content)\n","\n","download_file(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\", \"squad_train.json\")\n","download_file(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\", \"squad_dev.json\")\n","\n","def load_squad_data():\n","    with open('squad_train.json', 'r') as f:\n","        squad_data = json.load(f)\n","\n","    src_data = []\n","    trg_data = []\n","\n","    for article in squad_data['data']:\n","        for paragraph in article['paragraphs']:\n","            for qa in paragraph['qas']:\n","                if not qa['is_impossible']:\n","                    question = qa['question']\n","                    answer = qa['answers'][0]['text']\n","                    src_data.append(question)\n","                    trg_data.append(answer)\n","\n","    return src_data, trg_data\n","\n","SRC, TRG = load_squad_data()\n","SRC_train, SRC_test, TRG_train, TRG_test = train_test_split(SRC, TRG, test_size=0.2, random_state=42)\n","\n","train_df = pd.DataFrame({'src': SRC_train, 'trg': TRG_train})\n","test_df = pd.DataFrame({'src': SRC_test, 'trg': TRG_test})\n","\n","train_df.to_csv('train.csv', index=False)\n","test_df.to_csv('test.csv', index=False)\n","\n","SRC_TEXT = Field(tokenize=prepare_text, init_token='<sos>', eos_token='<eos>', lower=True)\n","TRG_TEXT = Field(tokenize=prepare_text, init_token='<sos>', eos_token='<eos>', lower=True)\n","\n","data_fields = [('src', SRC_TEXT), ('trg', TRG_TEXT)]\n","\n","def create_dataset(df, fields):\n","    examples = []\n","    for index, row in df.iterrows():\n","        src_data = row['src']\n","        trg_data = row['trg']\n","        examples.append(Example.fromlist([src_data, trg_data], fields))\n","    return Dataset(examples, fields)\n","\n","train_data = create_dataset(train_df, data_fields)\n","test_data = create_dataset(test_df, data_fields)\n","\n","SRC_TEXT.build_vocab(train_data, min_freq=2)\n","TRG_TEXT.build_vocab(train_data, min_freq=2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-39K069Kmiz7","executionInfo":{"status":"ok","timestamp":1680219256552,"user_tz":-120,"elapsed":29871,"user":{"displayName":"tech team","userId":"10297070148189427994"}},"outputId":"2e14cf44-6e06-4d8b-e500-34b3a4dabe30"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["# Encoder\n","class Encoder(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0):\n","        super(Encoder, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.dropout = dropout\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout)\n","\n","    def forward(self, src):\n","        embedded = self.embedding(src)\n","        output, (hidden, cell) = self.lstm(embedded)\n","        return hidden, cell"],"metadata":{"id":"x3nOu38_Yc4Z","executionInfo":{"status":"ok","timestamp":1680219256552,"user_tz":-120,"elapsed":7,"user":{"displayName":"tech team","userId":"10297070148189427994"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Decoder\n","class Decoder(nn.Module):\n","    def __init__(self, hidden_size, output_size, num_layers=1, dropout=0):\n","        super(Decoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.num_layers = num_layers\n","        self.dropout = dropout\n","\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout)\n","        self.fc_out = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, input, hidden, cell):\n","        input = input.unsqueeze(0)\n","        embedded = self.embedding(input)\n","        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n","        prediction = self.fc_out(output.squeeze(0))\n","        return prediction, hidden, cell"],"metadata":{"id":"2XTPtcxgZayf","executionInfo":{"status":"ok","timestamp":1680219256553,"user_tz":-120,"elapsed":7,"user":{"displayName":"tech team","userId":"10297070148189427994"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Seq2Seq\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        batch_size = trg.shape[1]\n","        trg_len = trg.shape[0]\n","        trg_vocab_size = len(TRG_TEXT.vocab)\n","\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        hidden, cell = self.encoder(src)\n","\n","        input = trg[0, :]\n","\n","        for t in range(1, trg_len):\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","            outputs[t] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.argmax(1)\n","            input = trg[t] if teacher_force else top1\n","\n","        return outputs"],"metadata":{"id":"dUE2tMaYaAiH","executionInfo":{"status":"ok","timestamp":1680219256553,"user_tz":-120,"elapsed":7,"user":{"displayName":"tech team","userId":"10297070148189427994"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!pip install tqdm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q92wXRQtjABw","executionInfo":{"status":"ok","timestamp":1680219260227,"user_tz":-120,"elapsed":3680,"user":{"displayName":"tech team","userId":"10297070148189427994"}},"outputId":"1303b5d4-91fc-4181-f958-c6ecc5f6a5b1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","import random\n","\n","# Hyperparameters\n","INPUT_DIM = len(SRC_TEXT.vocab)\n","OUTPUT_DIM = len(TRG_TEXT.vocab)\n","ENC_EMB_DIM = 256\n","DEC_EMB_DIM = 256\n","HID_DIM = 512\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5\n","NUM_EPOCHS = 20\n","LEARNING_RATE = 0.001\n","# Create iterators\n","BATCH_SIZE = 128\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","train_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, test_data),\n","    batch_size=BATCH_SIZE,\n","    sort_within_batch=True,\n","    sort_key=lambda x: len(x.src),\n","    device=device)\n","\n","\n","# Initialize models\n","enc = Encoder(INPUT_DIM, HID_DIM, dropout=ENC_DROPOUT)\n","dec = Decoder(HID_DIM, OUTPUT_DIM, dropout=DEC_DROPOUT)\n","model = Seq2Seq(enc, dec, device).to(device)\n","\n","# Optimizer\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","# Loss function\n","TRG_PAD_IDX = TRG_TEXT.vocab.stoi[TRG_TEXT.pad_token]\n","criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n","\n","# Training loop\n","for epoch in range(NUM_EPOCHS):\n","    epoch_loss = 0\n","\n","    # I Wrapped the train_iterator with tqdm to display a progress bar\n","    for idx, batch in enumerate(tqdm(train_iterator)):\n","        src = batch.src\n","        trg = batch.trg\n","\n","        optimizer.zero_grad()\n","\n","        output = model(src, trg)\n","        output = output[1:].view(-1, output.shape[-1])\n","        trg = trg[1:].view(-1)\n","\n","        loss = criterion(output, trg)\n","        loss.backward()\n","\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}: Loss = {epoch_loss/len(train_iterator)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wxzLHyK_aF8M","executionInfo":{"status":"ok","timestamp":1680237081394,"user_tz":-120,"elapsed":6100008,"user":{"displayName":"tech team","userId":"10297070148189427994"}},"outputId":"cf8ead57-e520-475f-f9a9-4a81996bd051"},"execution_count":9,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","100%|██████████| 543/543 [14:09<00:00,  1.56s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1: Loss = 6.0158536289278315\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 543/543 [15:30<00:00,  1.71s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 2: Loss = 5.404767302518391\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 543/543 [15:32<00:00,  1.72s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 3: Loss = 5.058805553513557\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 543/543 [15:33<00:00,  1.72s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 4: Loss = 4.736509864062455\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 543/543 [15:50<00:00,  1.75s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 5: Loss = 4.389767016275592\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 543/543 [16:20<00:00,  1.81s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 6: Loss = 4.033947415553843\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 543/543 [16:13<00:00,  1.79s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 7: Loss = 3.6694505759146114\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 543/543 [14:31<00:00,  1.60s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 8: Loss = 3.3013825842469218\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 543/543 [14:36<00:00,  1.61s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 9: Loss = 2.8908930601994633\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 543/543 [14:34<00:00,  1.61s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 10: Loss = 2.5242633924958455\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 543/543 [14:26<00:00,  1.60s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 11: Loss = 2.1248148664365596\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 543/543 [14:27<00:00,  1.60s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 12: Loss = 1.785863956474248\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 543/543 [13:37<00:00,  1.51s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13: Loss = 1.462346934481879\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 543/543 [13:57<00:00,  1.54s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14: Loss = 1.1754933632978857\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 543/543 [13:58<00:00,  1.55s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15: Loss = 0.9318665676573583\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 543/543 [16:17<00:00,  1.80s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16: Loss = 0.7209270115715364\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 543/543 [14:08<00:00,  1.56s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17: Loss = 0.540498335168287\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 543/543 [14:02<00:00,  1.55s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18: Loss = 0.41438500931688876\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 543/543 [14:34<00:00,  1.61s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19: Loss = 0.30131793951077135\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 543/543 [14:33<00:00,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 20: Loss = 0.22775825866348837\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["def evaluate(model, iterator, criterion):\n","    model.eval()\n","    epoch_loss = 0\n","\n","    with torch.no_grad():\n","        for _, batch in enumerate(iterator):\n","            src = batch.src\n","            trg = batch.trg\n","\n","            output = model(src, trg, 0)  \n","            output = output[1:].view(-1, output.shape[-1])\n","            trg = trg[1:].view(-1)\n","\n","            loss = criterion(output, trg)\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n","\n","test_loss = evaluate(model, test_iterator, criterion)\n","print(f\"Test Loss: {test_loss}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dpfvJW1UaUJo","executionInfo":{"status":"ok","timestamp":1680237116451,"user_tz":-120,"elapsed":35059,"user":{"displayName":"tech team","userId":"10297070148189427994"}},"outputId":"7f6e7bb8-c589-423b-92bf-29d190b8872b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 8.253702857915092\n"]}]},{"cell_type":"code","source":["def text_to_tensor(text, field, device):\n","    tokens = field.tokenize(text)\n","    indexes = [field.vocab.stoi[token] for token in tokens]\n","    tensor = torch.LongTensor(indexes).to(device)\n","    return indexes, tensor\n","\n","\n","def generate_response(model, sentence, src_field, trg_field, device, max_len=50):\n","    model.eval()\n","    \n","    src_indexes, src_tensor = text_to_tensor(sentence, src_field, device)\n","    src_tensor = src_tensor.unsqueeze(1)\n","\n","    with torch.no_grad():\n","        hidden, cell = model.encoder(src_tensor)\n","\n","    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n","    for _ in range(max_len):\n","        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)  \n","        with torch.no_grad():\n","            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)  \n","        pred_token = output.argmax(1).item()\n","        trg_indexes.append(pred_token)\n","        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n","            break\n","\n","    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n","    return ' '.join(trg_tokens[1:-1])  \n"],"metadata":{"id":"R4pTPZvzAidS","executionInfo":{"status":"ok","timestamp":1680239435970,"user_tz":-120,"elapsed":442,"user":{"displayName":"tech team","userId":"10297070148189427994"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["import traceback\n","\n","def test_bot():\n","    while True:\n","        try:\n","            input_sentence = input(\"Enter a question or type 'exit' to quit: \")\n","            if input_sentence.lower() == \"exit\":\n","                break\n","            response = generate_response(model, input_sentence, SRC_TEXT, TRG_TEXT, device)\n","            print(f\"Answer: {response}\")\n","        except Exception as e:\n","            print(f\"Error: {e}\")\n","            traceback.print_exc()  \n","\n","test_bot()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7MW0T-rBz6-","executionInfo":{"status":"ok","timestamp":1680241288161,"user_tz":-120,"elapsed":57398,"user":{"displayName":"tech team","userId":"10297070148189427994"}},"outputId":"774f26b2-1ecf-4a05-f41f-bd13a0fb7b11"},"execution_count":56,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter a question or type 'exit' to quit: Prior to Kennedy v. Louisiana, how many states criminalized child rape?\n","Answer: three\n","Enter a question or type 'exit' to quit: In what year did Louis die?\n","Answer: <unk>\n","Enter a question or type 'exit' to quit: How many people attended the service in Lviv?\n","Answer: around 280 , 000\n","Enter a question or type 'exit' to quit: exit\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1aMt40ua-69NnhZCYAm6oIk6Eq8e5aPJn","timestamp":1680171005084}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"accelerator":"TPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}